{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络模型是由神经网络层和Tensor操作构成的，`mindspore.nn` 提供了常见神经网络层的实现，在MindSpore中，`Cell`类是构建所有网络的基类，也是网络的基本单元。\n",
    "\n",
    "下面我们将构建一个用于Mnist数据集分类的神经网络模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn, ops\n",
    "\n",
    "#mindspore.nn: 提供了构建神经网络的层和组件，类似于torch.nn和tf.keras.layers\n",
    "#mindspore.ops: 提供了底层操作和函数式API，类似于torch.nn.functional+torch操作和tf.nn+tf.math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 定义模型类\n",
    "\n",
    "当我们定义神经网络时，可以继承`nn.Cell`类，在`__init__`方法中进行子Cell的实例化和状态管理，在`construct`方法中实现Tensor操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Network(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() \n",
    "        #创建一个扁平化层，用于将输入的多维数据（如图像）展平为一维向量，28*28=784\n",
    "        \n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            #nn.SequentialCell 是一个容器，按顺序包含多个网络层，数据按顺序通过这些层\n",
    "\n",
    "            nn.Dense(28*28, 512, weight_init=\"normal\", bias_init=\"zeros\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 512, weight_init=\"normal\", bias_init=\"zeros\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(512, 10, weight_init=\"normal\", bias_init=\"zeros\")\n",
    "        )\n",
    "\n",
    "    def construct(self, x):  \n",
    "        #construct是MindSpore中定义网络前向传播的方法（等同于PyTorch中的forward\n",
    "        x = self.flatten(x)\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "构建完成后，实例化`Network`对象，并查看其结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们构造一个输入数据，直接调用模型，可以获得一个十维的Tensor输出，其包含每个类别的原始预测值。\n",
    "\n",
    "tips: `model.construct()`方法不可直接调用。\n",
    "- 使用`model(inputs)`形式调用模型，这会间接调用`construct()`\n",
    "- 使用MindSpore提供的高级API如`model.train`、`model.predict`等会自动调用`construct()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.3765442e-03  6.6814530e-03  2.0682917e-03  7.6120538e-03\n",
      "  -1.0852818e-06 -5.9628091e-03 -5.5858246e-03  1.0529639e-03\n",
      "   7.6235924e-03 -1.1235978e-03]]\n"
     ]
    }
   ],
   "source": [
    "X = ops.ones((1, 28, 28), mindspore.float32)\n",
    "logits = model(X)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此基础上，我们通过一个`nn.Softmax`层实例来获得预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [8]\n"
     ]
    }
   ],
   "source": [
    "pred_probab = nn.Softmax(axis=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "\n",
    "#训练的时候往往不会把nn.softmax()函数放进construct层，\n",
    "#一般会选择使用交叉熵损失函数，nn.SoftmaxCrossEntropyWithLogits已经在内部实现了Softmax操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型层\n",
    "\n",
    "我们分解上节神经网络模型中的每一层。\n",
    "\n",
    "首先我们构造一个shape为(3, 28, 28)的随机数据（3个28x28的图像），依次通过每一个神经网络层来观察其效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "input_image = ops.ones((3, 28, 28), mindspore.float32)\n",
    "print(input_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.Flatten\n",
    "\n",
    "实例化`nn.Flatten` 层，`nn.Flatten()` 将把除了第一个维度（通常是批量大小batch_size）之外的所有维度展平成一个维度。\n",
    "\n",
    "即，将28x28的2D张量转换为784大小的连续数组。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 784)\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "#flatten = nn.Flatten(start_dim=2)\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### nn.Dense\n",
    "\n",
    "`nn.Dense`为全连接层，使用weights和bias对输入进行线性变换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 20)\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Dense(in_channels=28*28, out_channels=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.ReLU\n",
    "\n",
    "`nn.ReLU`层给网络中加入非线性的激活函数，帮助神经网络学习各种复杂的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: [[-0.02011143 -0.26672617 -0.81789654 -0.04325772  0.8912562  -0.20275453\n",
      "   0.16646272 -0.42533413 -0.02069402 -0.40331644 -0.14965856  0.9365725\n",
      "  -0.82388407  0.25218958  0.6173708   0.18822476  0.57242197 -0.7200695\n",
      "  -0.28978503  0.06200574]\n",
      " [-0.02011143 -0.26672617 -0.81789654 -0.04325772  0.8912562  -0.20275453\n",
      "   0.16646272 -0.42533413 -0.02069402 -0.40331644 -0.14965856  0.9365725\n",
      "  -0.82388407  0.25218958  0.6173708   0.18822476  0.57242197 -0.7200695\n",
      "  -0.28978503  0.06200574]\n",
      " [-0.02011143 -0.26672617 -0.81789654 -0.04325772  0.8912562  -0.20275453\n",
      "   0.16646272 -0.42533413 -0.02069402 -0.40331644 -0.14965856  0.9365725\n",
      "  -0.82388407  0.25218958  0.6173708   0.18822476  0.57242197 -0.7200695\n",
      "  -0.28978503  0.06200574]]\n",
      "\n",
      "\n",
      "After ReLU: [[0.         0.         0.         0.         0.8912562  0.\n",
      "  0.16646272 0.         0.         0.         0.         0.9365725\n",
      "  0.         0.25218958 0.6173708  0.18822476 0.57242197 0.\n",
      "  0.         0.06200574]\n",
      " [0.         0.         0.         0.         0.8912562  0.\n",
      "  0.16646272 0.         0.         0.         0.         0.9365725\n",
      "  0.         0.25218958 0.6173708  0.18822476 0.57242197 0.\n",
      "  0.         0.06200574]\n",
      " [0.         0.         0.         0.         0.8912562  0.\n",
      "  0.16646272 0.         0.         0.         0.         0.9365725\n",
      "  0.         0.25218958 0.6173708  0.18822476 0.57242197 0.\n",
      "  0.         0.06200574]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### nn.SequentialCell\n",
    "\n",
    "`nn.SequentialCell`是一个有序的Cell容器。输入Tensor将按照定义的顺序通过所有Cell。\n",
    "\n",
    "我们可以使用`nn.SequentialCell`来快速组合构造一个神经网络模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.SequentialCell(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Dense(20, 10)\n",
    ")\n",
    "\n",
    "logits = seq_modules(input_image)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Softmax\n",
    "\n",
    "最后使用[nn.Softmax](https://www.mindspore.cn/docs/zh-CN/r2.3/api_python/nn/mindspore.nn.Softmax.html)将神经网络最后一个全连接层返回的logits的值缩放为\\[0, 1\\]，表示每个类别的预测概率。`axis`指定的维度数值和为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(axis=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型参数\n",
    "\n",
    "网络内部神经网络层具有权重参数和偏置参数（如`nn.Dense`），这些参数会在训练过程中不断进行优化。\n",
    "\n",
    "可通过 `model.parameters_and_names()` 来获取参数名及对应的参数详情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n",
      "\n",
      "\n",
      "Layer: dense_relu_sequential.0.weight\n",
      "Size: (512, 784)\n",
      "Values : [[ 0.00237562 -0.00517793  0.00126484 ...  0.00091744  0.01295729\n",
      "  -0.00779298]\n",
      " [ 0.01920481 -0.00843089  0.00683935 ... -0.0012791  -0.00246767\n",
      "  -0.00070447]] \n",
      "\n",
      "Layer: dense_relu_sequential.0.bias\n",
      "Size: (512,)\n",
      "Values : [0. 0.] \n",
      "\n",
      "Layer: dense_relu_sequential.2.weight\n",
      "Size: (512, 512)\n",
      "Values : [[-0.01328323  0.00447932 -0.00536488 ... -0.00459299 -0.00993784\n",
      "  -0.0070957 ]\n",
      " [-0.00471527  0.0024026   0.00012553 ...  0.00692865 -0.00701123\n",
      "   0.00399537]] \n",
      "\n",
      "Layer: dense_relu_sequential.2.bias\n",
      "Size: (512,)\n",
      "Values : [0. 0.] \n",
      "\n",
      "Layer: dense_relu_sequential.4.weight\n",
      "Size: (10, 512)\n",
      "Values : [[ 0.01777492 -0.005901   -0.00964299 ... -0.01114079 -0.00779395\n",
      "   0.00541817]\n",
      " [ 0.0165071   0.00304458 -0.0084466  ... -0.00473582 -0.00907035\n",
      "  -0.00463813]] \n",
      "\n",
      "Layer: dense_relu_sequential.4.bias\n",
      "Size: (10,)\n",
      "Values : [0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.parameters_and_names():\n",
    "    print(f\"Layer: {name}\\nSize: {param.shape}\\nValues : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms2.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
