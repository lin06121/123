{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy基础 \n",
    "\n",
    "\n",
    "**完成此任务后，你将:**\n",
    "- 能够使用numpy函数和numpy矩阵/向量运算\n",
    "- 了解“广播”的概念\n",
    "- 了解代码“向量化”\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 用numpy构建基本函数 ##\n",
    "\n",
    "Numpy是Python中科学计算的主要软件包。它由一个大型社区（www.numpy.org)来维护。\n",
    "在本练习中，将学习一些关键的numpy函数，例如np.exp，np.log和np.reshape。\n",
    "同时，将了解如何在以后的任务中使用这些功能。\n",
    "\n",
    "### 1.1 - sigmoid 函数： np.exp() ###\n",
    "在使用np.exp（）之前，我们先使用math.exp（）实现Sigmoid函数。\n",
    "然后，你将知道为什么np.exp（）比math.exp（）更可取。\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "使用math.exp（x）构建一个参数为x，返回Sigmoid结果的函数。\n",
    "\n",
    "\n",
    "**Reminder**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ 也称为逻辑函数。它是一种非线性函数，不仅用于机器学习（逻辑回归），而且还用于深度学习。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:33:37.924461Z",
     "start_time": "2025-02-26T09:33:37.920319Z"
    }
   },
   "source": [
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s=1/(1+math.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，我们很少在深度学习中使用“math”库。因为函数的输入是实数，而深度学习中，我们需要使用矩阵和向量。 \n",
    "\n",
    "这就是为什么numpy更有用。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:33:40.591051Z",
     "start_time": "2025-02-26T09:33:40.419092Z"
    }
   },
   "source": [
    "### 在深度学习中，我们用numpy库代替math库的原因之一 ###\n",
    "x = [1, 2, 3]\n",
    "basic_sigmoid(x)  # x是向量，所以此处会报错"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m### 在深度学习中，我们用numpy库代替math库的原因之一 ###\u001B[39;00m\n\u001B[0;32m      2\u001B[0m x \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m----> 3\u001B[0m \u001B[43mbasic_sigmoid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# x是向量，所以此处会报错\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[1], line 17\u001B[0m, in \u001B[0;36mbasic_sigmoid\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124;03mCompute sigmoid of x.\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03ms -- sigmoid(x)\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m### START CODE HERE ### (≈ 1 line of code)\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m s\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39m(\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m+\u001B[39mmath\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mx\u001B[49m))\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m### END CODE HERE ###\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m s\n",
      "\u001B[1;31mTypeError\u001B[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上, 如果 $ x = (x_1, x_2, ..., x_n)$ 是个向量，那么 $np.exp(x)$ 将会把指数函数运算应用于向量中的每一个元素x. 输出将会是: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:33:43.243810Z",
     "start_time": "2025-02-26T09:33:43.193685Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# np中建立数组的方法np.array()\n",
    "x = np.array([1, 2, 3])\n",
    "\n",
    "print(np.exp(x)) # 结果将是(exp(1), exp(2), exp(3))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而且, 如果 x 是一个向量, 类似 $s = x + 3$ 或 $s = \\frac{1}{x}$ 这样的Python计算将会输出一个和x一样大小的向量。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# example of vector operation\n",
    "x = np.array([1, 2, 3])\n",
    "print (x + 3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要有关numpy函数的更多信息，建议查看[官方文档](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html).\n",
    "\n",
    "**Exercise**: 使用numpy实现`sigmoid()`。\n",
    "\n",
    "**Instructions**: x现在可以是实数、向量或矩阵。而我们在numpy中所使用的来表示这些矢量，矩阵等的数据结构称为numpy数组。\n",
    "\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:33:48.249076Z",
     "start_time": "2025-02-26T09:33:48.246404Z"
    }
   },
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "import numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s=1/(1+np.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid([1,2,3])**</td> \n",
    "        <td> array([ 0.73105858,  0.88079708,  0.95257413]) </td> \n",
    "    </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid gradient\n",
    "\n",
    "正如在基础知识里所提到的，我们将需要计算梯度来实现反向传播，优化损失函数。 \n",
    "\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "完成`sigmoid_grad（）`函数来计算sigmoid函数对x的梯度。可以使用前面定义好的`sigmoid()`函数。\n",
    "\n",
    "sigmoid函数的梯度公式为:\n",
    "\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:33:51.484139Z",
     "start_time": "2025-02-26T09:33:51.481561Z"
    }
   },
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    ds=sigmoid(x)*(1-sigmoid(x))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid_derivative([1,2,3])**</td> \n",
    "        <td> [ 0.19661193  0.10499359  0.04517666] </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "深度学习中，经常使用的两个numpy函数是[np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) 和 [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "\n",
    "- X.shape用于获取矩阵/向量X的形状（尺寸）。\n",
    "- X.reshape（...）用于将X重塑为其他的形状。\n",
    "\n",
    "举个例子：在计算机科学中，一张图片是由shape$（length，height，dipth=3）$的3D数组来表示。 但当我们读取图片作为算法输入时，会将它转换为shape为$（length * height * 3，1）$的向量。也就是说，我们将3D的array“展开”或reshape为1D的vector。\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Exercise**: 完成 `image2vector()`，把输入shape为(length, height, 3)reshape成(length\\*height\\*3, 1).\n",
    "\n",
    "举个例子，如果我们需要把shape为 (a, b, c) 的输入reshape成 (a*b,c) 你需要使用:\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```\n",
    "- 注意：请不要将图片的尺寸设为常数，我们使用`image.shape [0]`索引的方法来查找所需的数量。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:33:54.998726Z",
     "start_time": "2025-02-26T09:33:54.996208Z"
    }
   },
   "source": [
    "# GRADED FUNCTION: image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v=image.reshape((image.shape[0])*(image.shape[1])*(image.shape[2]),1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <tr> \n",
    "       <td> **image2vector(image)** </td> \n",
    "       <td> [[ 0.67826139]\n",
    " [ 0.29380381]\n",
    " [ 0.90714982]\n",
    " [ 0.52835647]\n",
    " [ 0.4215251 ]\n",
    " [ 0.45017551]\n",
    " [ 0.92814219]\n",
    " [ 0.96677647]\n",
    " [ 0.85304703]\n",
    " [ 0.52351845]\n",
    " [ 0.19981397]\n",
    " [ 0.27417313]\n",
    " [ 0.60659855]\n",
    " [ 0.00533165]\n",
    " [ 0.10820313]\n",
    " [ 0.49978937]\n",
    " [ 0.34144279]\n",
    " [ 0.94630077]]</td> \n",
    "     </tr>\n",
    "    \n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Broadcasting and softmax ####\n",
    "\n",
    "在numpy中的一个非常重要的概念就是Broadcasting“广播”————它对于执行不同shape数组之间的运算非常有用。\n",
    "有关广播的详细信息，可以阅读[broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: 使用numpy实现`softmax()`。softmax常被视为处理多分类问题时的归一化函数。\n",
    "\n",
    "**Instructions**: \n",
    "\n",
    "- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{对于一个矩阵} x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ 代表 $i^{th}$ 行  $j^{th}$ 列的元素, 因此我们有: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:33:59.998883Z",
     "start_time": "2025-02-26T09:33:59.996162Z"
    }
   },
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    x_exp=np.exp(x)\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    x_sum=np.sum(x_exp,axis=1,keepdims=True)\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    s=x_exp/x_sum\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "     <tr> \n",
    "       <td> **softmax(x)** </td> \n",
    "       <td> [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n",
    "    1.21052389e-04]\n",
    " [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n",
    "    8.01252314e-04]]</td> \n",
    "     </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- 如果我们打印一下x_exp，x_sum和s的形状，将会看到x_sum的shape为（2,1），而x_exp和s的shape为（2,5）。 正是因为广播功能，x_exp/x_sum才能工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2) Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 实现 L1 和 L2 损失函数\n",
    "\n",
    "**Exercise**: 完成 L1 损失的 numpy 版本。np.abs(x)函数可用来计算绝对值。np.sum(x)可用于计算和\n",
    "\n",
    "**Reminder**:\n",
    "- 损失函数用于计算模型的性能，损失越大,说明预测值 ($ \\hat{y} $) 距离真实值 ($y$) 越远。在深度学习中，我们常常使用如 Gradient Descent 的优化算法来训练模型，降低损失。 \n",
    "- L1 损失函数如下：\n",
    "\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:34:03.910001Z",
     "start_time": "2025-02-26T09:34:03.907533Z"
    }
   },
   "source": [
    "# GRADED FUNCTION: L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss=np.sum(np.abs(y-yhat))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L1** </td> \n",
    "       <td> 1.1 </td> \n",
    "     </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: 完成L2损失的numpy向量化版本。实现方式很多，你可以使用np.dot( )函数。\n",
    "\n",
    "举个例子，如果 $x = [x_1, x_2, ..., x_n]$, 那么 `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- L2 损失函数如下：\n",
    "\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T09:34:06.773959Z",
     "start_time": "2025-02-26T09:34:06.771291Z"
    }
   },
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss=np.dot((y-yhat),(y-yhat).T)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L2** </td> \n",
    "       <td> 0.43 </td> \n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你完成Numpy的热身小练习！"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
